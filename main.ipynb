{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-geometric\n",
        "!pip install utils"
      ],
      "metadata": {
        "id": "LB6TMl5AUlKz",
        "outputId": "b95257bf-236a-4dee-c45a-112737c4625c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "LB6TMl5AUlKz",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp (from torch-geometric)\n",
            "  Downloading aiohttp-3.11.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2024.12.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->torch-geometric)\n",
            "  Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp->torch-geometric)\n",
            "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (24.3.0)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp->torch-geometric)\n",
            "  Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->torch-geometric)\n",
            "  Downloading multidict-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
            "Collecting propcache>=0.2.0 (from aiohttp->torch-geometric)\n",
            "  Downloading propcache-0.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp->torch-geometric)\n",
            "  Downloading yarl-1.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2024.12.14)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp-3.11.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
            "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (274 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.9/274.9 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multidict-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.0/129.0 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading propcache-0.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (231 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.1/231.1 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yarl-1.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (344 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m344.1/344.1 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: propcache, multidict, frozenlist, aiohappyeyeballs, yarl, aiosignal, aiohttp, torch-geometric\n",
            "Successfully installed aiohappyeyeballs-2.4.4 aiohttp-3.11.11 aiosignal-1.3.2 frozenlist-1.5.0 multidict-6.1.0 propcache-0.2.1 torch-geometric-2.6.1 yarl-1.18.3\n",
            "Collecting utils\n",
            "  Downloading utils-1.0.2.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: utils\n",
            "  Building wheel for utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for utils: filename=utils-1.0.2-py2.py3-none-any.whl size=13906 sha256=c7acdc44ee1c8172fc6c8b2ce57d6642d187812a9ef95e5c764d57516863aa56\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/0c/b3/674aea8c5d91c642c817d4d630bd58faa316724b136844094d\n",
            "Successfully built utils\n",
            "Installing collected packages: utils\n",
            "Successfully installed utils-1.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d08ea544-0986-47d3-895a-31efbdbf36eb",
      "metadata": {
        "id": "d08ea544-0986-47d3-895a-31efbdbf36eb"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pathlib import Path\n",
        "import argparse\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import random\n",
        "\n",
        "import torch_geometric\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch_geometric\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import DataLoader as graph_dataloader\n",
        "from torch_geometric.utils import dense_to_sparse, remove_self_loops\n",
        "import random\n",
        "\n",
        "def seed_everything(seed = 0):\n",
        "    r\"\"\"Sets the seed for generating random numbers in :pytorch:`PyTorch`,\n",
        "    :obj:`numpy` and Python.\n",
        "\n",
        "    Args:\n",
        "        seed (int): The desired seed.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "def normalize(df_train, df_val, df_test):\n",
        "    df_train_min = df_train.min()\n",
        "    df_train_max = df_train.max()\n",
        "\n",
        "    df_train_normalized = (df_train - df_train_min) / (df_train_max - df_train_min)\n",
        "    df_val_normalized = (df_val - df_train_min) / (df_train_max - df_train_min)\n",
        "    df_test_normalized = (df_test - df_train_min) / (df_train_max - df_train_min)\n",
        "\n",
        "\n",
        "    return df_train_normalized, df_val_normalized, df_test_normalized\n",
        "\n",
        "\n",
        "def split_dataframe(df, df_physics, train_ratio=0.8, val_ratio=0.1, mode='physics-enhanced'):\n",
        "        train_ratio = int(0.8 * len(df.index))\n",
        "        val_ratio = int(0.1 * len(df.index))\n",
        "        df_train = df.iloc[:train_ratio]\n",
        "        df_val = df.iloc[train_ratio:train_ratio+val_ratio]\n",
        "        df_test = df.iloc[train_ratio+val_ratio:]\n",
        "        df_train, df_val, df_test = normalize(df_train, df_val, df_test)\n",
        "        df_X_train = df_train.iloc[:,37:]\n",
        "        df_y_train = df_train.iloc[:,:37]\n",
        "        df_X_val = df_val.iloc[:,37:]\n",
        "        df_y_val = df_val.iloc[:,:37]\n",
        "        df_X_test = df_test.iloc[:,37:]\n",
        "        df_y_test = df_test.iloc[:,:37]\n",
        "\n",
        "        if mode == 'physics-enhanced':\n",
        "            df_physics_train = df_physics.iloc[:train_ratio]\n",
        "            df_physics_val = df_physics.iloc[train_ratio:train_ratio+val_ratio]\n",
        "            df_physics_test = df_physics.iloc[train_ratio+val_ratio:]\n",
        "            df_physics_train, df_physics_val, df_physics_test = normalize(df_physics_train, df_physics_val, df_physics_test)\n",
        "\n",
        "            df_X_train = pd.concat([df_X_train, df_physics_train], axis = 1)\n",
        "            df_X_val = pd.concat([df_X_val, df_physics_val], axis = 1)\n",
        "            df_X_test = pd.concat([df_X_test, df_physics_test], axis = 1)\n",
        "\n",
        "\n",
        "        return np.array(df_X_train), np.array(df_y_train), np.array(df_X_val), np.array(df_y_val), np.array(df_X_test), np.array(df_y_test)\n",
        "\n",
        "\n",
        "def gaussian_kernel_distance(feature1, feature2, sigma):\n",
        "    # Calculate the Euclidean distance between two feature vectors\n",
        "    distance = np.linalg.norm(feature1 - feature2)\n",
        "    # Apply the Gaussian kernel function\n",
        "    weight = np.exp(-distance**2 / (sigma**2))\n",
        "    return weight\n",
        "\n",
        "\n",
        "def construct_graph(dataset, threshold_factor=1.0):\n",
        "    num_features = dataset.shape[1]\n",
        "    adjacency_matrix = np.zeros((num_features, num_features))\n",
        "\n",
        "    # Calculate pairwise distances\n",
        "    pairwise_distances = np.zeros((num_features, num_features))\n",
        "    for i in range(num_features):\n",
        "        for j in range(i+1, num_features):\n",
        "            pairwise_distances[i, j] = np.linalg.norm(dataset[:, i] - dataset[:, j])\n",
        "            pairwise_distances[j, i] = pairwise_distances[i, j]\n",
        "\n",
        "    # Calculate sigma as a multiple of the standard deviation of distances\n",
        "    sigma = np.std(pairwise_distances) * threshold_factor\n",
        "\n",
        "    # Construct the adjacency matrix with thresholding\n",
        "    for i in range(num_features):\n",
        "        for j in range(i+1, num_features):\n",
        "            if pairwise_distances[i, j] <= threshold_factor:\n",
        "                weight = gaussian_kernel_distance(dataset[:, i], dataset[:, j], sigma)\n",
        "                adjacency_matrix[i, j] = weight\n",
        "                adjacency_matrix[j, i] = weight\n",
        "\n",
        "    return adjacency_matrix + np.identity(adjacency_matrix.shape[0])\n",
        "\n",
        "\n",
        "def construct_pyg_data(df_X, df_y, device, window_size = 8):\n",
        "    PyG_Data = []\n",
        "\n",
        "    for i in range(df_X.T.shape[1] - window_size):\n",
        "        start_idx = i\n",
        "        end_idx = start_idx + window_size\n",
        "\n",
        "        # Construct adjacency matrix and edge index\n",
        "        adj = torch.from_numpy(construct_graph(df_X[start_idx:end_idx, :]).astype(float))\n",
        "        edge_index = (adj > 0).nonzero().t()\n",
        "        row, col = edge_index\n",
        "        edge_weight = adj[row, col]\n",
        "\n",
        "        # Convert NumPy arrays to PyTorch tensors\n",
        "        x = torch.tensor(df_X.T[:, start_idx:end_idx], dtype=torch.float32)\n",
        "        y = torch.tensor(df_y.T[:, start_idx:end_idx], dtype=torch.float32)\n",
        "        edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
        "        edge_weight = torch.tensor(edge_weight, dtype=torch.float32)\n",
        "\n",
        "        # Create PyG Data object and append to list\n",
        "        data = Data(x=x, edge_index=edge_index, edge_attr=edge_weight, y=y).to(device)\n",
        "        PyG_Data.append(data)\n",
        "\n",
        "    return PyG_Data\n",
        "\n",
        "from pathlib import Path\n",
        "import argparse\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import random\n",
        "\n",
        "\n",
        "from utils import *\n",
        "%load /content/model.py\n",
        "%load /content/train_test.py\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def get_arguments():\n",
        "    parser = argparse.ArgumentParser(description=\"Physics-Enhanced GNN for Soft Sensing\",\n",
        "                                     add_help=False)\n",
        "\n",
        "    # Data\n",
        "    parser.add_argument(\"--data-dir\", type=str, default=\"data/\",\n",
        "                        help='Path to the data')\n",
        "\n",
        "    # Checkpoints\n",
        "    parser.add_argument(\"--exp-dir\", type=Path, default=\"exp/\",\n",
        "                        help='Path to the experiment folder, where all logs/checkpoints will be stored')\n",
        "\n",
        "    # Optim\n",
        "    parser.add_argument(\"--seed\", type=int, default=42,\n",
        "                        help='Seed for experiments')\n",
        "    parser.add_argument(\"--epochs\", type=int, default=25000,\n",
        "                        help='Number of epochs')\n",
        "    parser.add_argument(\"--batch-size\", type=int, default=64,\n",
        "                        help='Batch size')\n",
        "    parser.add_argument(\"--base-lr\", type=float, default=3e-4,\n",
        "                        help='Base Learning rate')\n",
        "    parser.add_argument(\"--window-size\", type=int, default=8,\n",
        "                        help='Window Size')\n",
        "    parser.add_argument(\"--patience\", type=int, default=200,\n",
        "                        help='patience for early stopping')\n",
        "\n",
        "    # Running\n",
        "    parser.add_argument(\"--num-workers\", type=int, default=1)\n",
        "    parser.add_argument('--device', default='cuda:1',\n",
        "                        help='device to use for training / testing')\n",
        "\n",
        "    return parser\n",
        "\n",
        "\n",
        "def seed_everything(seed = 0):\n",
        "    r\"\"\"Sets the seed for generating random numbers in :pytorch:`PyTorch`,\n",
        "    :obj:`numpy` and Python.\n",
        "\n",
        "    Args:\n",
        "        seed (int): The desired seed.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "def mains(args):\n",
        "    device = args.device if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    df = pd.read_csv('/content/df_sensors (1).csv')\n",
        "    df_physics = pd.read_csv('/content/df_physics (1).csv')\n",
        "\n",
        "    df_X_train, df_y_train, df_X_val, df_y_val, df_X_test, df_y_test = split_dataframe(df, df_physics, train_ratio=0.8, val_ratio=0.1, mode='physics-enhanced')\n",
        "\n",
        "\n",
        "    #Create Dataset\n",
        "    PyG_Data_Train = construct_pyg_data(df_X_train, df_y_train, device)\n",
        "    PyG_Data_Val = construct_pyg_data(df_X_val, df_y_val, device)\n",
        "    PyG_Data_Test = construct_pyg_data(df_X_test, df_y_test, device)\n",
        "\n",
        "    #Create Dataloader\n",
        "    Train_DATA = graph_dataloader(PyG_Data_Train, batch_size = 64, shuffle = False, drop_last = True)\n",
        "    Validation_DATA = graph_dataloader(PyG_Data_Val, batch_size = 64, shuffle = False, drop_last = True)\n",
        "    Test_DATA = graph_dataloader(PyG_Data_Test, shuffle = False)\n",
        "\n",
        "    #define Model\n",
        "    model = GNNModel().to(device)\n",
        "\n",
        "    trained_model = train_gnn_model(model, Train_DATA, Validation_DATA, device = device, window_size = args.window_size, patience = args.patience, EPOCHS = args.epochs, lr = args.base_lr)\n",
        "    preds_list, targets_list, mse = test_gnn_model(trained_model, Test_DATA, window_size = args.window_size, device = device)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(parents=[get_arguments()])\n",
        "    args = parser.parse_args([])\n",
        "\n",
        "    seed_everything(args.seed)\n",
        "    mains(args)\n",
        "    from torch import nn\n",
        "from torch.nn import Parameter\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from torch_geometric.nn import ChebConv, GATv2Conv, TransformerConv\n",
        "\n",
        "class GNNModel(nn.Module):\n",
        "    def __init__(self, input_dim = 8, hidden_dim = 16, out_dim = 8, num_heads = 5):\n",
        "        super().__init__()\n",
        "        self.GCN1 = GATv2Conv(input_dim, hidden_dim, heads = num_heads, add_self_loops = False)\n",
        "        self.GCN2 = GATv2Conv(num_heads*hidden_dim, hidden_dim, heads = num_heads, add_self_loops = False)\n",
        "        self.linear = nn.Linear(2* num_heads*hidden_dim, out_dim)\n",
        "        self.linear1 = nn.Linear(38, 128)\n",
        "        self.linear2 = nn.Linear(128 , 37)\n",
        "\n",
        "    def forward(self, data):\n",
        "      x, edge_index, edge_weight = data.x.float(), data.edge_index, data.edge_weight\n",
        "      length = x.shape[1]\n",
        "      x = F.selu(self.GCN1(x, edge_index, edge_weight))\n",
        "      x_prev1 = x\n",
        "      x = F.selu(self.GCN2(x, edge_index, edge_weight))\n",
        "      x = torch.cat([x_prev1, x], dim = 1)\n",
        "      x = F.selu(self.linear(x))\n",
        "      x = F.selu(self.linear1(x.view(-1, 38, length).permute(0 , 2, 1)))\n",
        "      x = self.linear2(x)\n",
        "      return x\n",
        "      import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import NAdam\n",
        "\n",
        "def train_gnn_model(model, Train_DATA, Validation_DATA, window_size, device, EPOCHS, lr, patience = 200):\n",
        "\n",
        "    # Initialize model and optimizer\n",
        "    optimizer = NAdam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Training parameters\n",
        "    best_loss = float('inf')\n",
        "    best_model = None\n",
        "    counter = 0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "\n",
        "        # Training loop\n",
        "        for idx, data in enumerate(Train_DATA):\n",
        "            optimizer.zero_grad()\n",
        "            out = model(data)\n",
        "            loss = F.mse_loss(out.squeeze(), data.y.float().view(-1, 37, window_size).permute(0, 2, 1).squeeze())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "        train_losses = np.array(train_losses)\n",
        "        recon_epoch_loss = np.sqrt(train_losses.mean())\n",
        "        total_epoch_loss = recon_epoch_loss\n",
        "\n",
        "        # Print progress at every 10 epochs\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch: {epoch+1}\")\n",
        "            print('Training Loss:', recon_epoch_loss)\n",
        "\n",
        "        val_losses = []\n",
        "\n",
        "        # Validation loop\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for idx, data in enumerate(Validation_DATA):\n",
        "                out = model(data)\n",
        "                val_loss = F.mse_loss(out.squeeze(), data.y.float().view(-1, 37, window_size).permute(0, 2, 1).squeeze())\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "        val_losses = np.array(val_losses)\n",
        "        recon_epoch_loss_eval = np.sqrt(val_losses.mean())\n",
        "        total_epoch_loss_eval = recon_epoch_loss_eval\n",
        "\n",
        "        # Print progress at every 10 epochs\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print('Validation Loss:', recon_epoch_loss_eval)\n",
        "\n",
        "        # Early stopping\n",
        "        if total_epoch_loss_eval < best_loss:\n",
        "            best_loss = total_epoch_loss_eval\n",
        "            best_model = model.state_dict()\n",
        "            counter = 0\n",
        "        else:\n",
        "            counter += 1\n",
        "            if counter >= patience:\n",
        "                print('Early stopping')\n",
        "                break\n",
        "\n",
        "    # Load the best model\n",
        "    model.load_state_dict(best_model)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def test_gnn_model(model, Test_DATA, window_size, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    preds_list = []\n",
        "    targets_list = []\n",
        "    cost = 0\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        for idx, data in enumerate(Test_DATA):\n",
        "            data = data.to(device)\n",
        "            y_pred = model(data).squeeze()\n",
        "            preds_list.append(y_pred.cpu().numpy())\n",
        "            targets_list.append(data.y.float().view(-1, 37, window_size).permute(0, 2, 1).squeeze().cpu().numpy())\n",
        "            cost += torch.mean((y_pred - data.y.float().view(-1, 37, window_size).permute(0, 2, 1).squeeze())**2).item()\n",
        "\n",
        "    cost = cost / (idx + 1)\n",
        "    mse = np.sqrt(cost)\n",
        "    print(f\"MSE: {mse:.6f}\")\n",
        "\n",
        "    return preds_list, targets_list, mse\n"
      ],
      "metadata": {
        "id": "XQJ50baXTvec",
        "outputId": "c1d29c65-2efb-43ce-f5a7-d43de9b5cfb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        }
      },
      "id": "XQJ50baXTvec",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'GNNModel' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-11f632c9cafb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0mseed_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0mmains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-11f632c9cafb>\u001b[0m in \u001b[0;36mmains\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;31m#define Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGNNModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_gnn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrain_DATA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValidation_DATA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_lr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'GNNModel' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9be1a45-f994-4818-a419-c1b022edb3bb",
      "metadata": {
        "id": "f9be1a45-f994-4818-a419-c1b022edb3bb"
      },
      "outputs": [],
      "source": [
        "def get_arguments():\n",
        "    parser = argparse.ArgumentParser(description=\"Physics-Enhanced GNN for Soft Sensing\",\n",
        "                                     add_help=False)\n",
        "\n",
        "    # Data\n",
        "    parser.add_argument(\"--data-dir\", type=str, default=\"data/\",\n",
        "                        help='Path to the data')\n",
        "\n",
        "    # Checkpoints\n",
        "    parser.add_argument(\"--exp-dir\", type=Path, default=\"exp/\",\n",
        "                        help='Path to the experiment folder, where all logs/checkpoints will be stored')\n",
        "\n",
        "    # Optim\n",
        "    parser.add_argument(\"--seed\", type=int, default=42,\n",
        "                        help='Seed for experiments')\n",
        "    parser.add_argument(\"--epochs\", type=int, default=25000,\n",
        "                        help='Number of epochs')\n",
        "    parser.add_argument(\"--batch-size\", type=int, default=64,\n",
        "                        help='Batch size')\n",
        "    parser.add_argument(\"--base-lr\", type=float, default=3e-4,\n",
        "                        help='Base Learning rate')\n",
        "    parser.add_argument(\"--window-size\", type=int, default=8,\n",
        "                        help='Window Size')\n",
        "    parser.add_argument(\"--patience\", type=int, default=200,\n",
        "                        help='patience for early stopping')\n",
        "\n",
        "    # Running\n",
        "    parser.add_argument(\"--num-workers\", type=int, default=1)\n",
        "    parser.add_argument('--device', default='cuda:1',\n",
        "                        help='device to use for training / testing')\n",
        "\n",
        "    return parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45063b9f-79ad-49d4-a28c-0c209b893aa9",
      "metadata": {
        "id": "45063b9f-79ad-49d4-a28c-0c209b893aa9"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed = 0):\n",
        "    r\"\"\"Sets the seed for generating random numbers in :pytorch:`PyTorch`,\n",
        "    :obj:`numpy` and Python.\n",
        "\n",
        "    Args:\n",
        "        seed (int): The desired seed.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9f47d84-08cc-44e0-b4e5-1479074212a0",
      "metadata": {
        "id": "b9f47d84-08cc-44e0-b4e5-1479074212a0"
      },
      "outputs": [],
      "source": [
        "def mains(args):\n",
        "    device = args.device if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    df = pd.read_csv('/content/df_sensors (1).csv')\n",
        "    df_physics = pd.read_csv('Data/df_physics.csv')\n",
        "\n",
        "    df_X_train, df_y_train, df_X_val, df_y_val, df_X_test, df_y_test = split_dataframe(df, df_physics, train_ratio=0.8, val_ratio=0.1, mode='physics-enhanced')\n",
        "\n",
        "\n",
        "    #Create Dataset\n",
        "    PyG_Data_Train = construct_pyg_data(df_X_train, df_y_train, device)\n",
        "    PyG_Data_Val = construct_pyg_data(df_X_val, df_y_val, device)\n",
        "    PyG_Data_Test = construct_pyg_data(df_X_test, df_y_test, device)\n",
        "\n",
        "    #Create Dataloader\n",
        "    Train_DATA = graph_dataloader(PyG_Data_Train, batch_size = 64, shuffle = False, drop_last = True)\n",
        "    Validation_DATA = graph_dataloader(PyG_Data_Val, batch_size = 64, shuffle = False, drop_last = True)\n",
        "    Test_DATA = graph_dataloader(PyG_Data_Test, shuffle = False)\n",
        "\n",
        "    #define Model\n",
        "    model = GNNModel().to(device)\n",
        "\n",
        "    trained_model = train_gnn_model(model, Train_DATA, Validation_DATA, device = device, window_size = args.window_size, patience = args.patience, EPOCHS = args.epochs, lr = args.base_lr)\n",
        "    preds_list, targets_list, mse = test_gnn_model(trained_model, Test_DATA, window_size = args.window_size, device = device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aec5ee1c-d72d-4c87-b4f8-fd967e2f8e96",
      "metadata": {
        "id": "aec5ee1c-d72d-4c87-b4f8-fd967e2f8e96"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(parents=[get_arguments()])\n",
        "    args = parser.parse_args([])\n",
        "\n",
        "    seed_everything(args.seed)\n",
        "    mains(args)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import warnings\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Parameter\n",
        "from torch.optim import NAdam\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import DataLoader as graph_dataloader\n",
        "from torch_geometric.nn import GATv2Conv\n",
        "from torch_geometric.utils import dense_to_sparse, remove_self_loops\n",
        "\n",
        "# Ignore warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ---------------------- Utility Functions ----------------------\n",
        "\n",
        "def seed_everything(seed=0):\n",
        "    \"\"\"Sets the seed for generating random numbers in various libraries.\"\"\"\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "def normalize(df_train, df_val, df_test):\n",
        "    \"\"\"Normalizes train, validation, and test datasets based on training data statistics.\"\"\"\n",
        "    df_train_min = df_train.min()\n",
        "    df_train_max = df_train.max()\n",
        "\n",
        "    df_train_normalized = (df_train - df_train_min) / (df_train_max - df_train_min)\n",
        "    df_val_normalized = (df_val - df_train_min) / (df_train_max - df_train_min)\n",
        "    df_test_normalized = (df_test - df_train_min) / (df_train_max - df_train_min)\n",
        "\n",
        "    return df_train_normalized, df_val_normalized, df_test_normalized\n",
        "\n",
        "def split_dataframe(df, df_physics, train_ratio=0.8, val_ratio=0.1, mode='physics-enhanced'):\n",
        "    \"\"\"Splits the dataframe into train, validation, and test sets.\"\"\"\n",
        "    train_size = int(train_ratio * len(df))\n",
        "    val_size = int(val_ratio * len(df))\n",
        "\n",
        "    df_train = df.iloc[:train_size]\n",
        "    df_val = df.iloc[train_size:train_size + val_size]\n",
        "    df_test = df.iloc[train_size + val_size:]\n",
        "\n",
        "    df_train, df_val, df_test = normalize(df_train, df_val, df_test)\n",
        "\n",
        "    df_X_train = df_train.iloc[:, 37:]\n",
        "    df_y_train = df_train.iloc[:, :37]\n",
        "    df_X_val = df_val.iloc[:, 37:]\n",
        "    df_y_val = df_val.iloc[:, :37]\n",
        "    df_X_test = df_test.iloc[:, 37:]\n",
        "    df_y_test = df_test.iloc[:, :37]\n",
        "\n",
        "    if mode == 'physics-enhanced':\n",
        "        df_physics_train = df_physics.iloc[:train_size]\n",
        "        df_physics_val = df_physics.iloc[train_size:train_size + val_size]\n",
        "        df_physics_test = df_physics.iloc[train_size + val_size:]\n",
        "\n",
        "        df_physics_train, df_physics_val, df_physics_test = normalize(\n",
        "            df_physics_train, df_physics_val, df_physics_test)\n",
        "\n",
        "        df_X_train = pd.concat([df_X_train, df_physics_train], axis=1)\n",
        "        df_X_val = pd.concat([df_X_val, df_physics_val], axis=1)\n",
        "        df_X_test = pd.concat([df_X_test, df_physics_test], axis=1)\n",
        "\n",
        "    return np.array(df_X_train), np.array(df_y_train), np.array(df_X_val), np.array(df_y_val), np.array(df_X_test), np.array(df_y_test)\n",
        "\n",
        "def gaussian_kernel_distance(feature1, feature2, sigma):\n",
        "    \"\"\"Computes the Gaussian kernel distance between two feature vectors.\"\"\"\n",
        "    distance = np.linalg.norm(feature1 - feature2)\n",
        "    weight = np.exp(-distance**2 / (sigma**2))\n",
        "    return weight\n",
        "\n",
        "def construct_graph(dataset, threshold_factor=1.0):\n",
        "    \"\"\"Constructs an adjacency matrix for the dataset based on a Gaussian kernel.\"\"\"\n",
        "    num_features = dataset.shape[1]\n",
        "    adjacency_matrix = np.zeros((num_features, num_features))\n",
        "\n",
        "    pairwise_distances = np.zeros((num_features, num_features))\n",
        "    for i in range(num_features):\n",
        "        for j in range(i + 1, num_features):\n",
        "            pairwise_distances[i, j] = np.linalg.norm(dataset[:, i] - dataset[:, j])\n",
        "            pairwise_distances[j, i] = pairwise_distances[i, j]\n",
        "\n",
        "    sigma = np.std(pairwise_distances) * threshold_factor\n",
        "\n",
        "    for i in range(num_features):\n",
        "        for j in range(i + 1, num_features):\n",
        "            if pairwise_distances[i, j] <= threshold_factor:\n",
        "                weight = gaussian_kernel_distance(dataset[:, i], dataset[:, j], sigma)\n",
        "                adjacency_matrix[i, j] = weight\n",
        "                adjacency_matrix[j, i] = weight\n",
        "\n",
        "    return adjacency_matrix + np.identity(adjacency_matrix.shape[0])\n",
        "\n",
        "def construct_pyg_data(df_X, df_y, device, window_size=8):\n",
        "    \"\"\"Constructs PyTorch Geometric Data objects for the dataset.\"\"\"\n",
        "    PyG_Data = []\n",
        "\n",
        "    for i in range(df_X.T.shape[1] - window_size):\n",
        "        start_idx = i\n",
        "        end_idx = start_idx + window_size\n",
        "\n",
        "        adj = torch.from_numpy(construct_graph(df_X[start_idx:end_idx, :]).astype(float))\n",
        "        edge_index = (adj > 0).nonzero().t()\n",
        "        row, col = edge_index\n",
        "        edge_weight = adj[row, col]\n",
        "\n",
        "        x = torch.tensor(df_X.T[:, start_idx:end_idx], dtype=torch.float32)\n",
        "        y = torch.tensor(df_y.T[:, start_idx:end_idx], dtype=torch.float32)\n",
        "        edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
        "        edge_weight = torch.tensor(edge_weight, dtype=torch.float32)\n",
        "\n",
        "        data = Data(x=x, edge_index=edge_index, edge_attr=edge_weight, y=y).to(device)\n",
        "        PyG_Data.append(data)\n",
        "\n",
        "    return PyG_Data\n",
        "\n",
        "# ---------------------- Model Definition ----------------------\n",
        "\n",
        "class GNNModel(nn.Module):\n",
        "    def __init__(self, input_dim=8, hidden_dim=16, out_dim=8, num_heads=5):\n",
        "        super().__init__()\n",
        "        self.GCN1 = GATv2Conv(input_dim, hidden_dim, heads=num_heads, add_self_loops=False)\n",
        "        self.GCN2 = GATv2Conv(num_heads * hidden_dim, hidden_dim, heads=num_heads, add_self_loops=False)\n",
        "        self.linear = nn.Linear(2 * num_heads * hidden_dim, out_dim)\n",
        "        self.linear1 = nn.Linear(38, 128)\n",
        "        self.linear2 = nn.Linear(128, 37)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, edge_weight = data.x.float(), data.edge_index, data.edge_weight\n",
        "        length = x.shape[1]\n",
        "        x = F.selu(self.GCN1(x, edge_index, edge_weight))\n",
        "        x_prev1 = x\n",
        "        x = F.selu(self.GCN2(x, edge_index, edge_weight))\n",
        "        x = torch.cat([x_prev1, x], dim=1)\n",
        "        x = F.selu(self.linear(x))\n",
        "        x = F.selu(self.linear1(x.view(-1, 38, length).permute(0, 2, 1)))\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "# ---------------------- Training and Evaluation ----------------------\n",
        "\n",
        "def train_gnn_model(model, Train_DATA, Validation_DATA, window_size, device, EPOCHS, lr, patience=200):\n",
        "    \"\"\"Trains the GNN model.\"\"\"\n",
        "    optimizer = NAdam(model.parameters(), lr=lr)\n",
        "    best_loss = float('inf')\n",
        "    best_model = None\n",
        "    counter = 0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "\n",
        "        for idx, data in enumerate(Train_DATA):\n",
        "            optimizer.zero_grad()\n",
        "            out = model(data)\n",
        "            loss = F.mse_loss(out.squeeze(), data.y.float().view(-1, 37, window_size).permute(0, 2, 1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "        avg_train_loss = np.mean(train_losses)\n",
        "\n",
        "        # Validation Loop\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data in Validation_DATA:\n",
        "                out = model(data)\n",
        "                loss = F.mse_loss(out.squeeze(), data.y.float().view(-1, 37, window_size).permute(0, 2, 1))\n",
        "                val_losses.append(loss.item())\n",
        "\n",
        "        avg_val_loss = np.mean(val_losses)\n",
        "\n",
        "        # Early Stopping\n",
        "        if avg_val_loss < best_loss:\n",
        "            best_loss\n"
      ],
      "metadata": {
        "id": "dJu1LDKgZIiQ"
      },
      "id": "dJu1LDKgZIiQ",
      "execution_count": 6,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}